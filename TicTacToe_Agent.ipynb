{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rznSDgbvGggG"
   },
   "source": [
    "## Tic-Tac-Toe Agent\n",
    "â€‹\n",
    "In this notebook, you will learn to build an RL agent (using Q-learning) that learns to play Numerical Tic-Tac-Toe with odd numbers. The environment is playing randomly with the agent, i.e. its strategy is to put an even number randomly in an empty cell. The following is the layout of the notebook:\n",
    "        - Defining epsilon-greedy strategy\n",
    "        - Tracking state-action pairs for convergence\n",
    "        - Define hyperparameters for the Q-learning algorithm\n",
    "        - Generating episode and applying Q-update equation\n",
    "        - Checking convergence in Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eDb8PxBGggH"
   },
   "source": [
    "#### Importing libraries\n",
    "Write the code to import Tic-Tac-Toe class from the environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SFNYceFGggJ"
   },
   "outputs": [],
   "source": [
    "from TCGame_Env1 import TicTacToe #import your class from environment file\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, nan, nan]\n",
      "[3, 4, 5]\n",
      "[6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "env = TicTacToe()\n",
    "\n",
    "tempState = [0,float('nan'),float('nan'),3,4,5,6,7,8]\n",
    "for count in range(3):\n",
    "    row_list = [value for index,value in enumerate(tempState) if index>=count*3 and index<3+count*3]\n",
    "    print(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "([0, nan, nan, 3, 4, nan, 6, 8, 2], -10, True)\n"
     ]
    }
   ],
   "source": [
    "print(env.is_winning(tempState))\n",
    "\n",
    "testList = [0,float('nan'),float('nan'),3,4,float('nan'),6,float('nan'),float('nan')]\n",
    "print(env.step(testList, [7, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice([i for i in env.action_space(testList)[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYLQyopEG8nz"
   },
   "outputs": [],
   "source": [
    "# Function to convert state array into a string to store it as keys in the dictionary\n",
    "# states in Q-dictionary will be of form: x-4-5-3-8-x-x-x-x\n",
    "#   x | 4 | 5\n",
    "#   ----------\n",
    "#   3 | 8 | x\n",
    "#   ----------\n",
    "#   x | x | x\n",
    "\n",
    "def Q_state(state):\n",
    "    return ('-'.join(str(e) for e in state)).replace('nan','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZebMOoiVHBBr"
   },
   "outputs": [],
   "source": [
    "# Defining a function which will return valid (all possible actions) actions corresponding to a state\n",
    "# Important to avoid errors during deployment.\n",
    "\n",
    "def valid_actions(state):\n",
    "    valid_Actions = []\n",
    "    valid_Actions = [i for i in env.action_space(state)[0]] ###### -------please call your environment as env\n",
    "    return valid_Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRciPUkYHDWf"
   },
   "outputs": [],
   "source": [
    "# Defining a function which will add new Q-values to the Q-dictionary. \n",
    "def add_to_dict(state):\n",
    "    state1 = Q_state(state)\n",
    "    valid_act = valid_actions(state)\n",
    "    if state1 not in Q_dict.keys():\n",
    "        for action in valid_act:\n",
    "            Q_dict[state1][action]=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNNi_EfHGggM"
   },
   "source": [
    "#### Epsilon-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0lMfqiJGggN"
   },
   "outputs": [],
   "source": [
    "# Defining epsilon-greedy policy.\n",
    "def epsilon_greedy(state, episode):\n",
    "    max_epsilon = 1.0\n",
    "    min_epsilon = 0.001\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.0001*episode)\n",
    "    z = np.random.random()  \n",
    "    if z > epsilon:\n",
    "        action = max(Q_dict[Q_state(state)],key=Q_dict[Q_state(state)].get)  \n",
    "    else:\n",
    "        action = random.choice([i for i in env.action_space(state)[0]])\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2kyQHOMGggR"
   },
   "source": [
    "#### Tracking the state-action pairs for checking convergence - write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcxZ29vdGggS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initialise Q_dictionary as 'Q_dict' and States_tracked as 'States_track' (for convergence)\n",
    "Q_dict = collections.defaultdict(dict)\n",
    "States_track = collections.defaultdict(dict)\n",
    "Rewards = collections.defaultdict(dict)\n",
    "\n",
    "print(len(Q_dict))\n",
    "print(len(States_track))\n",
    "print(len(Rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs73iv8fHOxV"
   },
   "outputs": [],
   "source": [
    "# Initialise states to be tracked\n",
    "rewards_tracked = {('4-x-x-x-x-x-x-x-3',(1,1)):[],('x-9-x-x-x-4-x-x-x',(0,1)):[], ('x-x-x-x-x-x-x-x-x',(4, 9)): [], ('x-x-x-x-x-8-7-x-x',(1, 5)):[], ('4-x-x-x-7-x-x-6-3',(2, 1)):[]}\n",
    "def initialise_tracking_states():\n",
    "    sample_q_values = [('4-x-x-x-x-x-x-x-3',(1,1)),\n",
    "                       ('x-9-x-x-x-4-x-x-x',(0,1)),           \n",
    "                       ('x-x-x-x-x-x-x-x-x',(4,9)),\n",
    "                       ('x-x-x-x-x-8-7-x-x',(1,5)),\n",
    "                       ('4-x-x-x-7-x-x-6-3',(2,1))]\n",
    "    for q_values in sample_q_values:\n",
    "        state = q_values[0]\n",
    "        action = q_values[1]\n",
    "        States_track[state][action] = []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAbwJDMVHpwl"
   },
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Pyj7nMVHsBi"
   },
   "outputs": [],
   "source": [
    "def save_tracking_states():\n",
    "    for state in States_track.keys():\n",
    "        for action in States_track[state].keys():\n",
    "            if state in Q_dict and action in Q_dict[Q_state(state)]:\n",
    "                States_track[state][action].append(Q_dict[Q_state(state)][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_8xSluUHvew"
   },
   "outputs": [],
   "source": [
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iPt--E9GggV"
   },
   "source": [
    "#### Define hyperparameters  ---write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0_f5czFGggW"
   },
   "outputs": [],
   "source": [
    "#Defining parameters for the experiment\n",
    "EPISODES = 100000\n",
    "LR = 0.4                 #learning rate\n",
    "GAMMA = 0.91\n",
    "\n",
    "threshold = 20       #every these many episodes, the 4 Q-values will be stored/appended (convergence graphs)\n",
    "policy_threshold = 30    #every these many episodes, the Q-dict will be updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md6twJ7wGggh"
   },
   "source": [
    "### Q-update loop ---write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldCgQuDNGggj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10000 games, Agent Wins : 0.4998, Environment Wins : 0.5002, Tie : 0.0000\n",
      " 20000 games, Agent Wins : 0.5361, Environment Wins : 0.4639, Tie : 0.0000\n",
      " 30000 games, Agent Wins : 0.5708, Environment Wins : 0.4292, Tie : 0.0000\n",
      " 40000 games, Agent Wins : 0.6022, Environment Wins : 0.3978, Tie : 0.0000\n",
      " 50000 games, Agent Wins : 0.6225, Environment Wins : 0.3775, Tie : 0.0000\n",
      " 60000 games, Agent Wins : 0.6347, Environment Wins : 0.3654, Tie : 0.0000\n",
      " 70000 games, Agent Wins : 0.6481, Environment Wins : 0.3519, Tie : 0.0000\n",
      " 80000 games, Agent Wins : 0.6632, Environment Wins : 0.3368, Tie : 0.0000\n",
      " 90000 games, Agent Wins : 0.6811, Environment Wins : 0.3189, Tie : 0.0000\n",
      " 100000 games, Agent Wins : 0.6999, Environment Wins : 0.3001, Tie : 0.0000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "agent_win = 0\n",
    "env_win = 0\n",
    "tie = 0\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    ##### Start writing your code from the next line\n",
    "    time_step = 0\n",
    "    reward = None\n",
    "    total_reward = 0\n",
    "    TERMINAL = False\n",
    "\n",
    "    initial_state = copy.deepcopy(env.state)\n",
    "    curr_state = copy.deepcopy(env.state)  \n",
    "    add_to_dict(curr_state)\n",
    "\n",
    "    while not(TERMINAL):        \n",
    "        # Agent Selecting a Epsilon Greedy action\n",
    "        curr_action = epsilon_greedy(curr_state, episode)\n",
    "        next_state, reward, TERMINAL = env.step(curr_state, curr_action)\n",
    "        add_to_dict(next_state)\n",
    "\n",
    "        # UPDATE RULE\n",
    "        max_next = max(Q_dict[Q_state(curr_state)],key=Q_dict[Q_state(curr_state)].get)   \n",
    "        Q_dict[Q_state(curr_state)][curr_action] += LR * ((reward + (GAMMA*(Q_dict[Q_state(curr_state)][max_next]))) - Q_dict[Q_state(curr_state)][curr_action])\n",
    "\n",
    "        curr_state = next_state\n",
    "        \n",
    "        total_reward += reward\n",
    "\n",
    "        if TERMINAL:            \n",
    "            # Tracking the count of games won by agent and environment\n",
    "            if reward == -10:\n",
    "                env_win = env_win +1   \n",
    "            elif reward == 10:\n",
    "                agent_win = agent_win +1\n",
    "            elif reward == 0:\n",
    "                tie = tie+1\n",
    "            if (episode + 1) % (EPISODES*0.1) == 0:\n",
    "                print(\" %d games, Agent Wins : %.4f, Environment Wins : %.4f, Tie : %.4f\"% (episode+1, agent_win /(episode + 1), env_win /(episode+1), tie /(episode + 1)))\n",
    "                    \n",
    "    #TRACKING Q-VALUES\n",
    "    if (episode == threshold-1):        #at the 1999th episode\n",
    "        initialise_tracking_states()\n",
    "      \n",
    "    if ((episode+1) % threshold) == 0:   #every 2000th episode\n",
    "        save_tracking_states()\n",
    "        save_obj(States_track,'States_tracked')   \n",
    "    \n",
    "    #SAVING POLICY\n",
    "    if ((episode+1)% policy_threshold ) == 0: #every 30000th\n",
    "        save_obj(Q_dict,'Policy')    \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "save_obj(States_track,'States_tracked')   \n",
    "\n",
    "save_obj(Q_dict,'Policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LfSgVuHGggu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930.0692617893219"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "101708\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# with open('Rewards.pkl', 'rb') as handle:\n",
    "#     States_track = pickle.load(handle) \n",
    "with open('Policy.pkl', 'rb') as handle:\n",
    "    Q_dict = pickle.load(handle)\n",
    "with open('States_tracked.pkl', 'rb') as handle:\n",
    "    States_track = pickle.load(handle)    \n",
    "\n",
    "print(len(Rewards))    \n",
    "print(len(Q_dict))\n",
    "print(len(States_track))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6eMFbb8Ggg2"
   },
   "source": [
    "#### Check the Q-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr9d2fcVGgg4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-x-x-x-x-x-x-x-x : (0, 1) : -11.106672874138692\n",
      "x-x-x-x-x-x-x-x-x : (0, 3) : -11.10680434553889\n",
      "x-x-x-x-x-x-x-x-x : (0, 5) : -11.106683170969895\n",
      "x-x-x-x-x-x-x-x-x : (0, 7) : -11.10669893079669\n",
      "x-x-x-x-x-x-x-x-x : (0, 9) : -11.106686206392656\n",
      "x-x-x-x-x-x-x-x-x : (1, 1) : -11.106776028956501\n",
      "x-x-x-x-x-x-x-x-x : (1, 3) : -11.106703563905779\n",
      "x-x-x-x-x-x-x-x-x : (1, 5) : -11.106676595795129\n",
      "x-x-x-x-x-x-x-x-x : (1, 7) : -11.10667222073657\n",
      "x-x-x-x-x-x-x-x-x : (1, 9) : -11.106669616603554\n",
      "x-x-x-x-x-x-x-x-x : (2, 1) : -11.106701573882773\n",
      "x-x-x-x-x-x-x-x-x : (2, 3) : -11.106754127899992\n",
      "x-x-x-x-x-x-x-x-x : (2, 5) : -11.10680390333214\n",
      "x-x-x-x-x-x-x-x-x : (2, 7) : -11.106735058212507\n",
      "x-x-x-x-x-x-x-x-x : (2, 9) : -11.106730423519183\n",
      "x-x-x-x-x-x-x-x-x : (3, 1) : -11.106785675469013\n",
      "x-x-x-x-x-x-x-x-x : (3, 3) : -11.106684072941\n",
      "x-x-x-x-x-x-x-x-x : (3, 5) : -11.106765610669068\n",
      "x-x-x-x-x-x-x-x-x : (3, 7) : -11.10669836251737\n",
      "x-x-x-x-x-x-x-x-x : (3, 9) : -11.106788850215478\n",
      "x-x-x-x-x-x-x-x-x : (4, 1) : -11.106711848667826\n",
      "x-x-x-x-x-x-x-x-x : (4, 3) : -11.10674499087674\n",
      "x-x-x-x-x-x-x-x-x : (4, 5) : -11.106805778532898\n",
      "x-x-x-x-x-x-x-x-x : (4, 7) : -11.106739713750551\n",
      "x-x-x-x-x-x-x-x-x : (4, 9) : -11.106809134004145\n",
      "x-x-x-x-x-x-x-x-x : (5, 1) : -11.106799160123128\n",
      "x-x-x-x-x-x-x-x-x : (5, 3) : -11.106771304618775\n",
      "x-x-x-x-x-x-x-x-x : (5, 5) : -11.106785934723414\n",
      "x-x-x-x-x-x-x-x-x : (5, 7) : -11.106747794949953\n",
      "x-x-x-x-x-x-x-x-x : (5, 9) : -11.106763840619763\n",
      "x-x-x-x-x-x-x-x-x : (6, 1) : -11.106744973424782\n",
      "x-x-x-x-x-x-x-x-x : (6, 3) : -11.106717695370277\n",
      "x-x-x-x-x-x-x-x-x : (6, 5) : -11.106663231142402\n",
      "x-x-x-x-x-x-x-x-x : (6, 7) : -11.106663584617417\n",
      "x-x-x-x-x-x-x-x-x : (6, 9) : -11.106676540631168\n",
      "x-x-x-x-x-x-x-x-x : (7, 1) : -11.10665955241709\n",
      "x-x-x-x-x-x-x-x-x : (7, 3) : -11.106784431425833\n",
      "x-x-x-x-x-x-x-x-x : (7, 5) : -11.106775668831897\n",
      "x-x-x-x-x-x-x-x-x : (7, 7) : -11.106721120288727\n",
      "x-x-x-x-x-x-x-x-x : (7, 9) : -11.106651912486512\n",
      "x-x-x-x-x-x-x-x-x : (8, 1) : -11.10677309178128\n",
      "x-x-x-x-x-x-x-x-x : (8, 3) : -11.106797804189437\n",
      "x-x-x-x-x-x-x-x-x : (8, 5) : -11.106705465439056\n",
      "x-x-x-x-x-x-x-x-x : (8, 7) : -11.106673337164054\n",
      "x-x-x-x-x-x-x-x-x : (8, 9) : -11.106746193881127\n",
      "x-x-x-9-2-x-x-x-x : (0, 1) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (0, 3) : -3.0360000000000005\n",
      "x-x-x-9-2-x-x-x-x : (0, 5) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (0, 7) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (1, 1) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (1, 3) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (1, 5) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (1, 7) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (2, 1) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (2, 3) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (2, 5) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (2, 7) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (5, 1) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (5, 3) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (5, 5) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (5, 7) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (6, 1) : -3.2976\n",
      "x-x-x-9-2-x-x-x-x : (6, 3) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (6, 5) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (6, 7) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (7, 1) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (7, 3) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (7, 5) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (7, 7) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (8, 1) : 0.5640000000000001\n",
      "x-x-x-9-2-x-x-x-x : (8, 3) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (8, 5) : 1.0\n",
      "x-x-x-9-2-x-x-x-x : (8, 7) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (0, 3) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (0, 5) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (0, 7) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (1, 3) : 0.5640000000000001\n",
      "x-x-4-9-2-x-1-x-x : (1, 5) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (1, 7) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (5, 3) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (5, 5) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (5, 7) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (7, 3) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (7, 5) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (7, 7) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (8, 3) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (8, 5) : 1.0\n",
      "x-x-4-9-2-x-1-x-x : (8, 7) : 1.0\n",
      "x-3-4-9-2-x-1-x-6 : (0, 5) : 1.0\n",
      "x-3-4-9-2-x-1-x-6 : (0, 7) : 4.964\n",
      "x-3-4-9-2-x-1-x-6 : (5, 5) : 1.0\n",
      "x-3-4-9-2-x-1-x-6 : (5, 7) : 1.0\n",
      "x-3-4-9-2-x-1-x-6 : (7, 5) : 1.0\n",
      "x-3-4-9-2-x-1-x-6 : (7, 7) : 1.0\n",
      "7-3-4-9-2-x-1-x-6 : (5, 5) : 1.0\n",
      "7-3-4-9-2-x-1-x-6 : (7, 5) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (1, 3) : 0.5640000000000001\n",
      "6-x-x-x-x-1-x-x-x : (1, 5) : 0.5640000000000001\n",
      "6-x-x-x-x-1-x-x-x : (1, 7) : 0.5640000000000001\n",
      "6-x-x-x-x-1-x-x-x : (1, 9) : 0.5640000000000001\n",
      "6-x-x-x-x-1-x-x-x : (2, 3) : 0.5640000000000001\n",
      "6-x-x-x-x-1-x-x-x : (2, 5) : 0.5640000000000001\n",
      "6-x-x-x-x-1-x-x-x : (2, 7) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (2, 9) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (3, 3) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (3, 5) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (3, 7) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (3, 9) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (4, 3) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (4, 5) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (4, 7) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (4, 9) : 0.5640000000000001\n",
      "6-x-x-x-x-1-x-x-x : (6, 3) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (6, 5) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (6, 7) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (6, 9) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (7, 3) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (7, 5) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (7, 7) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (7, 9) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (8, 3) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (8, 5) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (8, 7) : 1.0\n",
      "6-x-x-x-x-1-x-x-x : (8, 9) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (1, 3) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (1, 5) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (1, 7) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (2, 3) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (2, 5) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (2, 7) : 4.964\n",
      "6-x-x-x-9-1-8-x-x : (3, 3) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (3, 5) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (3, 7) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (7, 3) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (7, 5) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (7, 7) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (8, 3) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (8, 5) : 1.0\n",
      "6-x-x-x-9-1-8-x-x : (8, 7) : 1.0\n",
      "6-x-7-x-9-1-8-x-x : (1, 3) : 1.0\n",
      "6-x-7-x-9-1-8-x-x : (1, 5) : 1.0\n",
      "6-x-7-x-9-1-8-x-x : (3, 3) : 1.0\n",
      "6-x-7-x-9-1-8-x-x : (3, 5) : 1.0\n",
      "6-x-7-x-9-1-8-x-x : (7, 3) : 1.0\n",
      "6-x-7-x-9-1-8-x-x : (7, 5) : 1.0\n",
      "6-x-7-x-9-1-8-x-x : (8, 3) : 1.0\n",
      "6-x-7-x-9-1-8-x-x : (8, 5) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (0, 1) : 0.5640000000000001\n",
      "x-x-x-x-x-8-x-3-x : (0, 5) : 0.5640000000000001\n",
      "x-x-x-x-x-8-x-3-x : (0, 7) : 0.5640000000000001\n",
      "x-x-x-x-x-8-x-3-x : (0, 9) : 0.5640000000000001\n",
      "x-x-x-x-x-8-x-3-x : (1, 1) : 0.5640000000000001\n",
      "x-x-x-x-x-8-x-3-x : (1, 5) : 0.5640000000000001\n",
      "x-x-x-x-x-8-x-3-x : (1, 7) : 0.5640000000000001\n",
      "x-x-x-x-x-8-x-3-x : (1, 9) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (2, 1) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (2, 5) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (2, 7) : 0.5640000000000001\n",
      "x-x-x-x-x-8-x-3-x : (2, 9) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (3, 1) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (3, 5) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (3, 7) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (3, 9) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (4, 1) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (4, 5) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (4, 7) : -3.0360000000000005\n",
      "x-x-x-x-x-8-x-3-x : (4, 9) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (6, 1) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (6, 5) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (6, 7) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (6, 9) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (8, 1) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (8, 5) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (8, 7) : 1.0\n",
      "x-x-x-x-x-8-x-3-x : (8, 9) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (0, 1) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (0, 5) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (0, 9) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (1, 1) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (1, 5) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (1, 9) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (2, 1) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (2, 5) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (2, 9) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (6, 1) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (6, 5) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (6, 9) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (8, 1) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (8, 5) : 1.0\n",
      "x-x-x-6-7-8-x-3-x : (8, 9) : 1.0\n"
     ]
    }
   ],
   "source": [
    "for index, data in enumerate(Q_dict.keys()):\n",
    "    if index < 10:\n",
    "        for i in Q_dict.get(data):\n",
    "            print(str(data)+' : '+str(i)+' : '+str(Q_dict.get(data)[i]))\n",
    "            # print(str(Q_dict.get(data)[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<class 'dict'>, {'x-3-x-x-x-6-x-x-x': {(0, 1): []}, 'x-1-x-x-x-x-8-x-x': {(2, 9): []}, 'x-x-x-x-6-x-x-x-5': {(2, 7): []}, 'x-x-x-x-9-x-6-x-x': {(1, 7): []}, 'x-5-x-2-x-x-4-7-x': {(0, 9): []}, '9-x-5-x-x-x-8-x-4': {(1, 3): []}, '2-7-x-x-6-x-x-3-x': {(8, 5): []}, '9-x-x-x-x-2-x-x-x': {(2, 5): []}, 'x-x-7-x-x-x-x-x-2': {(1, 5): []}, '5-x-x-x-x-6-x-x-x': {(4, 9): []}, '4-x-x-6-x-x-3-1-x': {(8, 5): []}, '5-x-8-x-x-6-3-x-x': {(3, 1): []}, 'x-6-5-x-2-x-x-3-x': {(0, 7): []}, '7-x-5-x-2-x-x-x-6': {(1, 3): []}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "States_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# try checking for one of the states - that which action your agent thinks is the best  -----This will not be evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGPZEQDFGghG"
   },
   "source": [
    "#### Check the states tracked for Q-values convergence\n",
    "(non-evaluative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s1Tvz8HGghH"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Write the code for plotting the graphs for state-action pairs tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVQInsg7GghL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2Opp8_NITkC"
   },
   "source": [
    "### Epsilon - decay check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQ_D_JsuGghR"
   },
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.001*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "J7c2xADQGghV",
    "outputId": "cb60fce3-570b-45fb-bd83-abde3d13b273"
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59BRf43IJiQ1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the states tracked for Q-values convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "In  \u001b[0;34m[73]\u001b[0m:\nLine \u001b[0;34m3\u001b[0m:     plt.subplot(\u001b[34m241\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 9)\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.figure(0, figsize=(16,7))\n",
    "plt.subplot(241)\n",
    "t1=States_track['x-x-8-7-x-5-6-3-x'][(0,9)]\n",
    "plt.title(\"(s,a)=('x-x-8-7-x-5-6-3-x',(0,9))\")\n",
    "plt.plot(np.asarray(range(0, len(t1))),np.asarray(t1))\n",
    "\n",
    "plt.subplot(242)\n",
    "t2=States_track['6-x-9-x-x-3-2-x-7'][(1, 5)]\n",
    "plt.title(\"(s,a)=('6-x-9-x-x-3-2-x-7',(1, 5))\")\n",
    "plt.plot(np.asarray(range(0, len(t2))),np.asarray(t2))\n",
    "\n",
    "plt.subplot(243)\n",
    "t3=States_track['x-x-2-6-x-5-9-x-x'][(0, 7)]\n",
    "plt.title(\"(s,a)=('x-x-2-6-x-5-9-x-x',(0, 7))\")\n",
    "plt.plot(np.asarray(range(0, len(t3))),np.asarray(t3))\n",
    "\n",
    "plt.subplot(244)\n",
    "t4=States_track['x-x-x-7-x-4-x-x-x'][(1, 1)]\n",
    "plt.title(\"(s,a)=('x-x-x-7-x-4-x-x-x',(1, 1))\")\n",
    "plt.plot(np.asarray(range(0, len(t4))),np.asarray(t4))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToe_Agent.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.7 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
